"""ADO Integration Implementation Validator

This script validates the complete ADO REST API integration implementation
for Story 2.2, checking all deliverables and requirements.

Validation Areas:
1. REST Client Implementation
2. Work Item Operations
3. Query Capabilities
4. Performance Optimizations
5. Monitoring and Error Handling
6. Test Coverage
7. Enterprise Scalability

Usage:
    python validate_implementation.py [--detailed] [--export-report]
"""

import asyncio
import inspect
import json
import sys
from datetime import datetime
from pathlib import Path
from typing import Dict, Any, List, Optional, Callable
import importlib.util

# Setup logging
import logging
logging.basicConfig(level=logging.INFO, format='%(levelname)s: %(message)s')
logger = logging.getLogger(__name__)


class ValidationResult:
    """Container for validation results."""
    
    def __init__(self, component: str):
        self.component = component
        self.passed = []
        self.failed = []
        self.warnings = []
        self.score = 0.0
        self.max_score = 0.0
    
    def add_pass(self, check: str, points: float = 1.0):
        """Add a passing validation check."""
        self.passed.append(check)
        self.score += points
        self.max_score += points
    
    def add_fail(self, check: str, reason: str, points: float = 1.0):
        """Add a failing validation check."""
        self.failed.append(f"{check}: {reason}")
        self.max_score += points
    
    def add_warning(self, warning: str):
        """Add a validation warning."""
        self.warnings.append(warning)
    
    @property
    def success_rate(self) -> float:
        """Calculate success rate as percentage."""
        if self.max_score == 0:
            return 100.0
        return (self.score / self.max_score) * 100


class ADOImplementationValidator:
    """Validates ADO integration implementation against Story 2.2 requirements."""
    
    def __init__(self):
        self.results = {}
        self.base_path = Path(__file__).parent
        
    def validate_all(self) -> Dict[str, ValidationResult]:
        """Run all validation checks."""
        logger.info("üîç Starting ADO integration implementation validation...")
        
        # Component validations
        self.results["rest_client"] = self.validate_rest_client()
        self.results["work_operations"] = self.validate_work_item_operations()
        self.results["query_capabilities"] = self.validate_query_capabilities()
        self.results["performance"] = self.validate_performance_optimizations()
        self.results["monitoring"] = self.validate_monitoring_error_handling()
        self.results["tests"] = self.validate_test_coverage()
        self.results["scalability"] = self.validate_enterprise_scalability()
        
        return self.results
    
    def validate_rest_client(self) -> ValidationResult:
        """Validate REST client implementation."""
        result = ValidationResult("REST API Client")
        
        try:
            from datascience_platform.qvf.ado.rest_client import (
                ADORestClient, ADOClientConfig, ADOApiError,
                ADOAuthenticationError, ADOPermissionError, 
                ADORateLimitError, ADOTimeoutError, RateLimiter
            )
            result.add_pass("‚úÖ All required classes imported successfully", 2.0)
        except ImportError as e:
            result.add_fail("‚ùå Import failed", str(e), 2.0)
            return result
        
        # Validate ADOClientConfig
        try:
            config = ADOClientConfig(
                organization_url="https://dev.azure.com/test",
                personal_access_token="test_token"
            )
            result.add_pass("‚úÖ ADOClientConfig creates successfully", 1.0)
            
            # Test configuration properties
            if hasattr(config, 'timeout_seconds') and config.timeout_seconds == 30:
                result.add_pass("‚úÖ Default timeout configured (30s)", 0.5)
            
            if hasattr(config, 'max_retries') and config.max_retries == 3:
                result.add_pass("‚úÖ Default retry count configured (3)", 0.5)
            
            if hasattr(config, 'max_concurrent_requests'):
                result.add_pass("‚úÖ Concurrent request limiting configured", 0.5)
            
            # Test methods
            if hasattr(config, 'get_auth_header'):
                auth_header = config.get_auth_header()
                if auth_header.startswith("Basic "):
                    result.add_pass("‚úÖ Authentication header generation working", 1.0)
                else:
                    result.add_fail("‚ùå Auth header format", "Header doesn't start with 'Basic '", 1.0)
            
            if hasattr(config, 'get_api_url'):
                url = config.get_api_url("wit/fields", "TestProject")
                if "TestProject" in url and "api-version" in url:
                    result.add_pass("‚úÖ API URL generation working", 1.0)
                else:
                    result.add_fail("‚ùå API URL format", "Missing project or api-version", 1.0)
        
        except Exception as e:
            result.add_fail("‚ùå ADOClientConfig validation", str(e), 4.0)
        
        # Validate ADORestClient class structure
        try:
            client_methods = [
                'start_session', 'close_session', '_make_request',
                'get_project', 'list_projects', 'list_work_item_fields',
                'get_work_item_field', 'create_work_item_field', 'update_work_item_field',
                'get_work_item', 'get_work_items_batch', 'update_work_item',
                'create_work_item', 'query_work_items', 'update_work_items_batch'
            ]
            
            missing_methods = []
            for method in client_methods:
                if not hasattr(ADORestClient, method):
                    missing_methods.append(method)
            
            if not missing_methods:
                result.add_pass("‚úÖ All required REST client methods present", 3.0)
            else:
                result.add_fail("‚ùå Missing methods", f"Missing: {', '.join(missing_methods)}", 3.0)
            
            # Check for async context manager support
            if hasattr(ADORestClient, '__aenter__') and hasattr(ADORestClient, '__aexit__'):
                result.add_pass("‚úÖ Async context manager support", 1.0)
            else:
                result.add_fail("‚ùå Missing async context manager", "Missing __aenter__/__aexit__", 1.0)
        
        except Exception as e:
            result.add_fail("‚ùå REST client structure validation", str(e), 4.0)
        
        # Validate exception hierarchy
        exception_classes = [
            ADOApiError, ADOAuthenticationError, ADOPermissionError,
            ADORateLimitError, ADOTimeoutError
        ]
        
        try:
            for exc_class in exception_classes:
                if issubclass(exc_class, Exception):
                    result.add_pass(f"‚úÖ {exc_class.__name__} properly inherits from Exception", 0.2)
                else:
                    result.add_fail(f"‚ùå {exc_class.__name__}", "Doesn't inherit from Exception", 0.2)
        except Exception as e:
            result.add_fail("‚ùå Exception hierarchy validation", str(e), 1.0)
        
        # Validate RateLimiter
        try:
            limiter = RateLimiter(requests_per_minute=100)
            if hasattr(limiter, 'acquire') and asyncio.iscoroutinefunction(limiter.acquire):
                result.add_pass("‚úÖ Rate limiter with async acquire method", 1.0)
            else:
                result.add_fail("‚ùå Rate limiter", "Missing or non-async acquire method", 1.0)
        except Exception as e:
            result.add_fail("‚ùå Rate limiter validation", str(e), 1.0)
        
        return result
    
    def validate_work_item_operations(self) -> ValidationResult:
        """Validate work item operations implementation."""
        result = ValidationResult("Work Item Operations")
        
        try:
            from datascience_platform.qvf.ado.work_items import (
                WorkItemManager, QVFWorkItemScore, WorkItemUpdateBatch,
                UpdateResult, WorkItemManagementError
            )
            result.add_pass("‚úÖ Work item classes imported successfully", 2.0)
        except ImportError as e:
            result.add_fail("‚ùå Work items import failed", str(e), 2.0)
            return result
        
        # Validate QVFWorkItemScore
        try:
            score = QVFWorkItemScore(
                work_item_id=123,
                overall_score=0.85,
                configuration_id="test"
            )
            
            if hasattr(score, 'to_field_updates'):
                field_updates = score.to_field_updates()
                if isinstance(field_updates, dict) and "Custom.QVFScore" in field_updates:
                    result.add_pass("‚úÖ QVF score to field updates conversion", 1.5)
                else:
                    result.add_fail("‚ùå Field updates format", "Missing Custom.QVFScore or wrong format", 1.5)
            
            if hasattr(score, 'get_score_summary'):
                summary = score.get_score_summary()
                if isinstance(summary, str) and len(summary) > 0:
                    result.add_pass("‚úÖ Score summary generation", 0.5)
                else:
                    result.add_fail("‚ùå Score summary", "Empty or invalid summary", 0.5)
        
        except Exception as e:
            result.add_fail("‚ùå QVFWorkItemScore validation", str(e), 2.0)
        
        # Validate WorkItemManager
        try:
            manager_methods = [
                'load_work_items_for_scoring', 'update_work_item_scores',
                'get_work_item_qvf_history', 'validate_qvf_setup',
                'get_operation_statistics'
            ]
            
            missing_methods = []
            for method in manager_methods:
                if not hasattr(WorkItemManager, method):
                    missing_methods.append(method)
            
            if not missing_methods:
                result.add_pass("‚úÖ All WorkItemManager methods present", 2.0)
            else:
                result.add_fail("‚ùå Missing WorkItemManager methods", f"Missing: {', '.join(missing_methods)}", 2.0)
            
            # Check async context manager
            if hasattr(WorkItemManager, '__aenter__'):
                result.add_pass("‚úÖ WorkItemManager async context manager support", 1.0)
            else:
                result.add_fail("‚ùå Missing async context manager", "WorkItemManager needs __aenter__", 1.0)
        
        except Exception as e:
            result.add_fail("‚ùå WorkItemManager validation", str(e), 3.0)
        
        # Validate batch processing classes
        try:
            batch = WorkItemUpdateBatch(
                batch_id="test",
                project_name="Test",
                work_item_scores={}
            )
            
            if hasattr(batch, 'mark_started') and hasattr(batch, 'mark_completed'):
                result.add_pass("‚úÖ Batch lifecycle management methods", 1.0)
            
            if hasattr(batch, 'success_rate') and hasattr(batch, 'processing_duration'):
                result.add_pass("‚úÖ Batch metrics calculation", 1.0)
        
        except Exception as e:
            result.add_fail("‚ùå Batch processing validation", str(e), 2.0)
        
        return result
    
    def validate_query_capabilities(self) -> ValidationResult:
        """Validate query builder and WIQL capabilities."""
        result = ValidationResult("Query Capabilities")
        
        try:
            from datascience_platform.qvf.ado.rest_client import ADORestClient
            
            # Check if query_work_items method exists
            if hasattr(ADORestClient, 'query_work_items'):
                result.add_pass("‚úÖ WIQL query execution method present", 2.0)
                
                # Check method signature
                sig = inspect.signature(ADORestClient.query_work_items)
                params = list(sig.parameters.keys())
                
                required_params = ['self', 'project_name', 'wiql_query']
                if all(param in params for param in required_params):
                    result.add_pass("‚úÖ Query method has required parameters", 1.0)
                else:
                    result.add_fail("‚ùå Query method parameters", "Missing required parameters", 1.0)
                
                if 'max_results' in params:
                    result.add_pass("‚úÖ Pagination support (max_results parameter)", 1.0)
                else:
                    result.add_warning("‚ö†Ô∏è No explicit pagination parameter found")
            else:
                result.add_fail("‚ùå Missing WIQL query capability", "No query_work_items method", 2.0)
            
            # Check batch retrieval
            if hasattr(ADORestClient, 'get_work_items_batch'):
                result.add_pass("‚úÖ Batch work item retrieval method present", 1.5)
            else:
                result.add_fail("‚ùå Missing batch retrieval", "No get_work_items_batch method", 1.5)
            
            # Check filtering capabilities in WorkItemManager
            from datascience_platform.qvf.ado.work_items import WorkItemManager
            
            if hasattr(WorkItemManager, 'load_work_items_for_scoring'):
                sig = inspect.signature(WorkItemManager.load_work_items_for_scoring)
                params = list(sig.parameters.keys())
                
                filter_params = ['work_item_types', 'states', 'area_path', 'iteration_path']
                present_filters = [p for p in filter_params if p in params]
                
                if len(present_filters) >= 3:
                    result.add_pass(f"‚úÖ Advanced filtering support ({len(present_filters)}/4 filters)", 2.0)
                else:
                    result.add_fail("‚ùå Limited filtering", f"Only {len(present_filters)}/4 filters supported", 2.0)
        
        except Exception as e:
            result.add_fail("‚ùå Query capabilities validation", str(e), 7.5)
        
        return result
    
    def validate_performance_optimizations(self) -> ValidationResult:
        """Validate performance optimization features."""
        result = ValidationResult("Performance Optimizations")
        
        try:
            from datascience_platform.qvf.ado.rest_client import ADORestClient, ADOClientConfig, RateLimiter
            
            # Check connection pooling configuration
            config = ADOClientConfig(
                organization_url="https://dev.azure.com/test",
                personal_access_token="test"
            )
            
            if hasattr(config, 'connection_pool_size'):
                result.add_pass("‚úÖ Connection pool size configuration", 1.0)
            
            if hasattr(config, 'max_concurrent_requests'):
                result.add_pass("‚úÖ Concurrent request limiting", 1.0)
            
            # Check rate limiting
            if hasattr(config, 'requests_per_minute'):
                result.add_pass("‚úÖ Rate limiting configuration", 1.0)
            
            # Check retry logic
            if hasattr(config, 'max_retries') and hasattr(config, 'retry_delay_seconds'):
                result.add_pass("‚úÖ Retry configuration with exponential backoff", 1.5)
            
            # Check batch processing
            from datascience_platform.qvf.ado.work_items import WorkItemManager
            
            if hasattr(WorkItemManager, 'update_work_items_batch'):
                result.add_pass("‚úÖ Batch update processing", 2.0)
            else:
                result.add_fail("‚ùå Missing batch processing", "No batch update method", 2.0)
            
            # Check async operations
            client_methods = ['get_work_item', 'update_work_item', 'query_work_items']
            async_methods = []
            
            for method_name in client_methods:
                if hasattr(ADORestClient, method_name):
                    method = getattr(ADORestClient, method_name)
                    if asyncio.iscoroutinefunction(method):
                        async_methods.append(method_name)
            
            if len(async_methods) == len(client_methods):
                result.add_pass("‚úÖ Full async/await support", 2.0)
            elif len(async_methods) > 0:
                result.add_pass(f"‚úÖ Partial async support ({len(async_methods)}/{len(client_methods)})", 1.0)
                result.add_warning("‚ö†Ô∏è Not all methods are async")
            else:
                result.add_fail("‚ùå No async support", "Methods are not async", 2.0)
            
            # Check caching/TTL features
            if hasattr(config, 'rate_limit_buffer'):
                result.add_pass("‚úÖ Rate limit buffering", 0.5)
        
        except Exception as e:
            result.add_fail("‚ùå Performance optimizations validation", str(e), 9.0)
        
        return result
    
    def validate_monitoring_error_handling(self) -> ValidationResult:
        """Validate monitoring and error handling capabilities."""
        result = ValidationResult("Monitoring & Error Handling")
        
        try:
            from datascience_platform.qvf.ado.rest_client import (
                ADOApiError, ADOAuthenticationError, ADOPermissionError,
                ADORateLimitError, ADOTimeoutError
            )
            
            # Validate comprehensive error hierarchy
            error_classes = [
                ADOApiError, ADOAuthenticationError, ADOPermissionError,
                ADORateLimitError, ADOTimeoutError
            ]
            
            for error_class in error_classes:
                if hasattr(error_class, '__init__'):
                    try:
                        # Test error instantiation
                        if error_class == ADORateLimitError:
                            error = error_class("Rate limited", retry_after=60)
                        else:
                            error = error_class("Test error", status_code=400)
                        result.add_pass(f"‚úÖ {error_class.__name__} properly implemented", 0.4)
                    except Exception:
                        result.add_fail(f"‚ùå {error_class.__name__}", "Cannot instantiate", 0.4)
                else:
                    result.add_fail(f"‚ùå {error_class.__name__}", "Missing __init__", 0.4)
            
            # Check performance metrics
            from datascience_platform.qvf.ado.rest_client import ADORestClient
            
            if hasattr(ADORestClient, 'get_performance_stats'):
                result.add_pass("‚úÖ Performance statistics tracking", 2.0)
            else:
                result.add_fail("‚ùå Missing performance metrics", "No get_performance_stats method", 2.0)
            
            # Check operation statistics in WorkItemManager
            from datascience_platform.qvf.ado.work_items import WorkItemManager
            
            if hasattr(WorkItemManager, 'get_operation_statistics'):
                result.add_pass("‚úÖ Operation statistics in WorkItemManager", 1.5)
            else:
                result.add_fail("‚ùå Missing operation statistics", "No statistics method", 1.5)
            
            # Check logging configuration
            import logging
            
            # Check if modules set up loggers
            rest_logger = logging.getLogger('datascience_platform.qvf.ado.rest_client')
            work_logger = logging.getLogger('datascience_platform.qvf.ado.work_items')
            
            if rest_logger and work_logger:
                result.add_pass("‚úÖ Module-level logging configured", 1.0)
            else:
                result.add_warning("‚ö†Ô∏è Logging configuration not verified")
            
            # Check request metrics tracking
            from datascience_platform.qvf.ado.rest_client import RequestMetrics
            
            if hasattr(RequestMetrics, 'duration_ms') and hasattr(RequestMetrics, 'complete'):
                result.add_pass("‚úÖ Request metrics tracking", 1.0)
            else:
                result.add_fail("‚ùå Missing request metrics", "RequestMetrics incomplete", 1.0)
        
        except Exception as e:
            result.add_fail("‚ùå Monitoring/error handling validation", str(e), 7.3)
        
        return result
    
    def validate_test_coverage(self) -> ValidationResult:
        """Validate test coverage and quality."""
        result = ValidationResult("Test Coverage")
        
        test_files = [
            "test_rest_client.py",
            "test_work_items.py",
            "run_integration_tests.py"
        ]
        
        tests_dir = self.base_path / "tests"
        
        for test_file in test_files:
            test_path = tests_dir / test_file
            if test_path.exists():
                result.add_pass(f"‚úÖ {test_file} exists", 1.0)
                
                # Check test file content
                content = test_path.read_text()
                
                if "def test_" in content:
                    test_count = content.count("def test_")
                    if test_count >= 5:
                        result.add_pass(f"‚úÖ {test_file} has comprehensive tests ({test_count} tests)", 1.0)
                    else:
                        result.add_warning(f"‚ö†Ô∏è {test_file} has limited tests ({test_count} tests)")
                        result.add_pass(f"‚úÖ {test_file} has some tests", 0.5)
                
                if "@pytest.mark.asyncio" in content:
                    result.add_pass(f"‚úÖ {test_file} includes async tests", 0.5)
                
                if "mock" in content.lower() or "Mock" in content:
                    result.add_pass(f"‚úÖ {test_file} uses mocking", 0.5)
            else:
                result.add_fail(f"‚ùå Missing {test_file}", "Test file not found", 1.0)
        
        # Check for test runner
        runner_path = tests_dir / "run_integration_tests.py"
        if runner_path.exists():
            content = runner_path.read_text()
            if "coverage" in content.lower():
                result.add_pass("‚úÖ Test runner with coverage support", 1.0)
            if "performance" in content.lower():
                result.add_pass("‚úÖ Performance benchmarking in test runner", 1.0)
        
        return result
    
    def validate_enterprise_scalability(self) -> ValidationResult:
        """Validate enterprise scalability features."""
        result = ValidationResult("Enterprise Scalability")
        
        try:
            from datascience_platform.qvf.ado.rest_client import ADOClientConfig
            from datascience_platform.qvf.ado.work_items import WorkItemManager
            
            # Check configuration scalability settings
            config = ADOClientConfig(
                organization_url="https://dev.azure.com/test",
                personal_access_token="test"
            )
            
            scalability_settings = [
                ('connection_pool_size', 20, 'Connection pooling'),
                ('max_concurrent_requests', 10, 'Concurrent request limiting'),
                ('requests_per_minute', 200, 'Rate limiting'),
                ('timeout_seconds', 30, 'Request timeout'),
                ('max_retries', 3, 'Retry configuration')
            ]
            
            for setting_name, expected_default, description in scalability_settings:
                if hasattr(config, setting_name):
                    value = getattr(config, setting_name)
                    if isinstance(value, (int, float)) and value > 0:
                        result.add_pass(f"‚úÖ {description} configured ({value})", 0.4)
                    else:
                        result.add_fail(f"‚ùå {description}", f"Invalid value: {value}", 0.4)
                else:
                    result.add_fail(f"‚ùå Missing {description}", f"No {setting_name} setting", 0.4)
            
            # Check batch processing capabilities
            manager_methods = [
                'update_work_items_batch',
                'load_work_items_for_scoring'
            ]
            
            for method in manager_methods:
                if hasattr(WorkItemManager, method):
                    sig = inspect.signature(getattr(WorkItemManager, method))
                    if 'batch_size' in str(sig):
                        result.add_pass(f"‚úÖ {method} supports batch sizing", 1.0)
                    else:
                        result.add_warning(f"‚ö†Ô∏è {method} may not support configurable batch sizing")
            
            # Check for 10,000+ work items support indicators
            if hasattr(WorkItemManager, '__init__'):
                sig = inspect.signature(WorkItemManager.__init__)
                if 'batch_size' in sig.parameters:
                    result.add_pass("‚úÖ Configurable batch size for large datasets", 1.5)
                else:
                    result.add_fail("‚ùå No configurable batch size", "May not scale to 10k+ items", 1.5)
            
            # Check error handling for enterprise scenarios
            from datascience_platform.qvf.ado.rest_client import ADORateLimitError, ADOTimeoutError
            
            if hasattr(ADORateLimitError, 'retry_after'):
                result.add_pass("‚úÖ Rate limit handling with retry timing", 1.0)
            
            if ADOTimeoutError:
                result.add_pass("‚úÖ Timeout error handling for large operations", 1.0)
        
        except Exception as e:
            result.add_fail("‚ùå Enterprise scalability validation", str(e), 8.3)
        
        return result
    
    def generate_report(self, detailed: bool = False) -> str:
        """Generate comprehensive validation report."""
        if not self.results:
            return "‚ùå No validation results available. Run validate_all() first."
        
        report_lines = [
            "="*80,
            "ADO INTEGRATION IMPLEMENTATION VALIDATION REPORT",
            "="*80,
            f"üìÖ Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}",
            f"üìã Story: 2.2 - ADO REST API Integration (10 SP)",
            "",
            "OVERALL SUMMARY:",
            "-"*20
        ]
        
        total_score = 0
        total_max_score = 0
        all_components_passing = True
        
        for component, result in self.results.items():
            total_score += result.score
            total_max_score += result.max_score
            
            status = "‚úÖ PASS" if result.success_rate >= 80 else "‚ùå FAIL"
            if result.success_rate < 80:
                all_components_passing = False
            
            report_lines.append(
                f"{status} {result.component}: {result.success_rate:.1f}% ({result.score:.1f}/{result.max_score:.1f})"
            )
        
        overall_score = (total_score / total_max_score * 100) if total_max_score > 0 else 0
        overall_status = "‚úÖ IMPLEMENTATION COMPLETE" if overall_score >= 80 else "‚ùå NEEDS WORK"
        
        report_lines.extend([
            "",
            f"üéØ OVERALL SCORE: {overall_score:.1f}% ({total_score:.1f}/{total_max_score:.1f})",
            f"üìä STATUS: {overall_status}",
            ""
        ])
        
        # Detailed component results
        if detailed:
            for component, result in self.results.items():
                report_lines.extend([
                    f"üìã {result.component.upper()}:",
                    "-" * (len(result.component) + 4)
                ])
                
                if result.passed:
                    report_lines.append("‚úÖ PASSED CHECKS:")
                    for check in result.passed:
                        report_lines.append(f"   {check}")
                    report_lines.append("")
                
                if result.failed:
                    report_lines.append("‚ùå FAILED CHECKS:")
                    for check in result.failed:
                        report_lines.append(f"   {check}")
                    report_lines.append("")
                
                if result.warnings:
                    report_lines.append("‚ö†Ô∏è  WARNINGS:")
                    for warning in result.warnings:
                        report_lines.append(f"   {warning}")
                    report_lines.append("")
        
        # Implementation completeness check
        report_lines.extend([
            "STORY 2.2 DELIVERABLE CHECKLIST:",
            "-"*35,
        ])
        
        deliverables = [
            ("REST API Client (rest_client.py)", "rest_client" in self.results),
            ("Work Item Operations", "work_operations" in self.results),
            ("Query Builder & WIQL Support", "query_capabilities" in self.results),
            ("Performance Optimizations", "performance" in self.results),
            ("Monitoring & Error Handling", "monitoring" in self.results),
            ("Comprehensive Test Coverage", "tests" in self.results),
            ("Enterprise Scalability (10k+ items)", "scalability" in self.results)
        ]
        
        completed_deliverables = 0
        for deliverable, completed in deliverables:
            status = "‚úÖ" if completed else "‚ùå"
            report_lines.append(f"{status} {deliverable}")
            if completed:
                completed_deliverables += 1
        
        completion_rate = (completed_deliverables / len(deliverables)) * 100
        report_lines.extend([
            "",
            f"üìä DELIVERABLE COMPLETION: {completion_rate:.0f}% ({completed_deliverables}/{len(deliverables)})",
            ""
        ])
        
        # Recommendations
        report_lines.extend([
            "RECOMMENDATIONS:",
            "-"*15
        ])
        
        if overall_score >= 90:
            report_lines.append("üéâ Excellent implementation! Ready for production deployment.")
        elif overall_score >= 80:
            report_lines.append("‚úÖ Good implementation. Address minor issues before deployment.")
        elif overall_score >= 60:
            report_lines.append("‚ö†Ô∏è  Implementation needs significant improvements.")
        else:
            report_lines.append("‚ùå Implementation requires major rework before deployment.")
        
        # Add specific recommendations based on failed components
        for component, result in self.results.items():
            if result.success_rate < 80 and result.failed:
                report_lines.append(f"üîß {result.component}: Focus on {len(result.failed)} failed checks")
        
        report_lines.extend([
            "",
            "="*80
        ])
        
        return "\n".join(report_lines)
    
    def export_report(self, filepath: str):
        """Export validation report to file."""
        report_content = self.generate_report(detailed=True)
        
        with open(filepath, 'w') as f:
            f.write(report_content)
        
        # Also export JSON results
        json_filepath = filepath.replace('.txt', '.json')
        json_data = {
            "validation_results": {
                component: {
                    "component": result.component,
                    "score": result.score,
                    "max_score": result.max_score,
                    "success_rate": result.success_rate,
                    "passed": result.passed,
                    "failed": result.failed,
                    "warnings": result.warnings
                }
                for component, result in self.results.items()
            },
            "overall_score": sum(r.score for r in self.results.values()) / sum(r.max_score for r in self.results.values()) * 100 if self.results else 0,
            "generated_at": datetime.now().isoformat()
        }
        
        with open(json_filepath, 'w') as f:
            json.dump(json_data, f, indent=2)
        
        logger.info(f"Reports exported to: {filepath} and {json_filepath}")


def main():
    """Main entry point for validation script."""
    import argparse
    
    parser = argparse.ArgumentParser(description="Validate ADO Integration Implementation")
    parser.add_argument("--detailed", "-d", action="store_true", 
                       help="Generate detailed report with all checks")
    parser.add_argument("--export-report", "-e", 
                       help="Export report to specified file path")
    
    args = parser.parse_args()
    
    # Create validator and run validation
    validator = ADOImplementationValidator()
    print("üîç Validating ADO integration implementation...")
    print("   This may take a moment while checking all components...")
    print()
    
    # Run validation
    results = validator.validate_all()
    
    # Generate and display report
    report = validator.generate_report(detailed=args.detailed)
    print(report)
    
    # Export if requested
    if args.export_report:
        validator.export_report(args.export_report)
    
    # Return exit code based on overall success
    overall_score = sum(r.score for r in results.values()) / sum(r.max_score for r in results.values()) * 100
    exit_code = 0 if overall_score >= 80 else 1
    
    return exit_code


if __name__ == "__main__":
    exit_code = main()
    sys.exit(exit_code)